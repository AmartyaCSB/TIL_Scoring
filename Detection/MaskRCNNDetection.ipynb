{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycocotools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycocotools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoco\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COCO\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycocotools'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    \"\"\"function to show an image\"\"\"\n",
    "    img = img/2 + 0.5\n",
    "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def display_data_sample(img, target):\n",
    "    \"\"\"function to display one data point\"\"\"\n",
    "    # display the image\n",
    "    imshow(img)\n",
    "    print()\n",
    "    # check for annotations\n",
    "    if len(target['boxes']):\n",
    "        # add all the masks and display\n",
    "        mask = np.sum(target['masks'].numpy(), axis=0).astype(np.uint8)\n",
    "        plt.imshow(mask, cmap='gray', vmin=0, vmax=255)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('no annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TILDetectionDataset(Dataset):\n",
    "    \"\"\"COCO Detection format dataset for TIL detection\"\"\"\n",
    "    \n",
    "    def __init__(self, root, annFile):\n",
    "        self.root = root\n",
    "        self.annFile = annFile\n",
    "        self.coco = COCO(annFile)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"return the number of images\"\"\"\n",
    "        return len(self.coco.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns (img, target)\n",
    "            img    : normalized image tensor\n",
    "            target : dict with keys :\n",
    "                            boxes (FloatTensor[N, 4])\n",
    "                            labels (Int64Tensor[N])\n",
    "                            masks (UInt8Tensor[N, H, W])\n",
    "        \"\"\"\n",
    "        \n",
    "        # read the image\n",
    "        filename = self.root + self.coco.imgs[idx]['file_name']\n",
    "        img = torchvision.io.read_image(filename)\n",
    "        img = torch.as_tensor(img, dtype=torch.float)\n",
    "        img = img/255\n",
    "        _, H,W = img.shape\n",
    "        # normalize the image\n",
    "        img = torchvision.transforms.functional.normalize(img, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        # get the annotations in img\n",
    "        annIds = self.coco.getAnnIds(imgIds=idx)\n",
    "        anns = self.coco.loadAnns(annIds)\n",
    "        # generate boxes\n",
    "        boxes = []\n",
    "        for ann in anns:\n",
    "            xmin = ann['bbox'][0]\n",
    "            ymin = ann['bbox'][1]\n",
    "            xmax = xmin + ann['bbox'][2]\n",
    "            ymax = ymin + ann['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # generate labels\n",
    "        labels = torch.ones((len(anns)), dtype=torch.int64)\n",
    "        # create blank image for mask\n",
    "        masks = np.zeros((len(anns), H, W), dtype=np.uint8)\n",
    "        # generate masks\n",
    "        for idx, ann in enumerate(anns):\n",
    "            xmin = ann['bbox'][0]\n",
    "            ymin = ann['bbox'][1]\n",
    "            xmax = xmin + ann['bbox'][2]\n",
    "            ymax = ymin + ann['bbox'][3]\n",
    "            masks[idx] = cv2.rectangle(masks[idx], (xmin, ymin), (xmax, ymax), 255,-1)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        # craete dictionary\n",
    "        retDict = {\n",
    "            'boxes':boxes,\n",
    "            'labels':labels,\n",
    "            'masks':masks}\n",
    "        return (img, retDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = TILDetectionDataset('/data/wsirois/roi-level-annotations/tissue-cells/', '/data/wsirois/roi-level-annotations/tissue-cells/tiger-coco.json')\n",
    "\n",
    "# create train and test split from the data\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = train_dataset[0]\n",
    "display_data_sample(img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr= 3e-4, weight_decay=0.01)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    \n",
    "    running_train_loss = 0.0\n",
    "    running_test_loss = 0.0\n",
    "\n",
    "    with tqdm(train_dataset, unit=\"batch\") as tepoch:\n",
    "        for i, (img, target) in enumerate(tepoch, 0):\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            \n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            if len(target['boxes']) > 0:\n",
    "                for key in target.keys():\n",
    "                    target[key] = target[key].to(device)\n",
    "                \n",
    "                img = img.to(device)\n",
    "                \n",
    "                target = [target]\n",
    "                img = [img]\n",
    "            \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(img, target)\n",
    "                loss = outputs['loss_classifier'].item() + outputs['loss_box_reg'].item() + outputs['loss_mask'].item() + outputs['loss_objectness'].item() + outputs['loss_rpn_box_reg']\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # detatch the loss from GPU\n",
    "                loss = loss.detach().cpu()\n",
    "                \n",
    "                # track statistics\n",
    "                #running_train_loss += loss.item()\n",
    "                #tepoch.set_postfix(loss=loss.item())\n",
    "                \n",
    "                # discard variables to free GPU memory\n",
    "                del outputs\n",
    "                del loss\n",
    "                del img\n",
    "                for key in target[0].keys():\n",
    "                    target[0][key] = target[0][key].detach().cpu()\n",
    "                \n",
    "                # empty the GPU cache\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    train_losses.append(running_train_loss/len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
